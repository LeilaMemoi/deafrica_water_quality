import json
import sys

import click
import numpy as np
from odc.stats._cli_common import click_yaml_cfg

from water_quality.grid import WaterbodiesGrid
from water_quality.hue import hue_calculation
from water_quality.instruments import check_instrument_dates, get_instruments_list
from water_quality.io import (
    check_directory_exists,
    check_file_exists,
    get_filesystem,
    join_url,
)
from water_quality.load_data import build_dc_queries, build_wq_dataset
from water_quality.logs import setup_logging
from water_quality.optical_water_type import OWT_pixel
from water_quality.pixel_corrections import R_correction
from water_quality.tiling import get_tile_index_int_tuple
from water_quality.water_detection import water_analysis
from water_quality.wq_algorithms import ALGORITHMS_CHLA, ALGORITHMS_TSM, WQ_vars

CONFIG_ITEMS = [
    "start_date",
    "end_date",
    "instruments_to_use",
    "water_frequency_threshold_high",
    "water_frequency_threshold_low",
    "permanent_water_threshold",
    "sigma_coefficient",
]


@click.command(
    name="process-tasks",
    no_args_is_help=True,
)
@click.argument(
    "input-file",
    type=str,
)
@click.argument(
    "output-directory",
    type=str,
)
@click.argument(
    "max-parallel-steps",
    type=int,
)
@click.argument(
    "worker-idx",
    type=int,
)
@click_yaml_cfg(
    "--analysis-config",
    required=True,
    help="Config for the analysis parameters in yaml format, file or text",
)
@click.option(
    "--overwrite/--no-overwrite",
    default=False,
    show_default=True,
    help=(
        "If overwrite is True tasks that have already been processed will be rerun. "
    ),
)
def cli(
    input_file: str,
    output_directory: str,
    max_parallel_steps: int,
    worker_idx: int,
    analysis_config,
    overwrite: bool,
):
    """
    Get the Water Quality variables for the input tiles and write the resulting water
    quality variables to netCDF files.

    INPUT_FILE: The path to the text file containing the tile ids for the tiles to be
    processed. The text file is generated by running the `wq-generate-tasks` command.

    OUTPUT_DIRECTORY: The directory to write the water quality variables NETCDF
    file to for each tile.

    MAX_PARALLEL_STEPS: The total number of parallel workers or pods expected in the workflow.
    This value is used to divide the list of input tiles among the available workers.

    WORKER_IDX: The sequential index (0-indexed) of the current worker.
    This index determines which subset of tiles the current worker will process.
    """
    log = setup_logging()

    # Expecting file to be public-read
    if not check_file_exists(input_file):
        raise FileNotFoundError(f"Input file {input_file} does not exist.")

    fs = get_filesystem(input_file, anon=True)
    with fs.open(input_file, "r") as file:
        # Read all lines and strip leading/trailing whitespace (including newlines)
        all_tile_ids = [line.strip() for line in file if line.strip()]

    # Split files equally among the workers
    task_chunks = np.array_split(np.array(all_tile_ids), max_parallel_steps)
    task_chunks = [chunk.tolist() for chunk in task_chunks]
    task_chunks = list(filter(None, task_chunks))

    # In case of the index being bigger than the number of positions in the array, the extra POD isn't necessary
    if len(task_chunks) <= worker_idx:
        log.warning(f"Worker {worker_idx} Skipped!")
        sys.exit(0)

    log.info(f"Executing worker {worker_idx}")
    tile_ids = task_chunks[worker_idx]
    tile_ids.sort()
    log.info(f"Worker {worker_idx} to process {len(tile_ids)} tasks.")

    # Verify parameters config
    if analysis_config is None:
        raise ValueError(
            "Please provide a config for the analysis parameters in yaml format, file or text"
        )

    missing_parameters = []
    for k in CONFIG_ITEMS:
        if k not in list(analysis_config.keys()):
            missing_parameters.append(k)
    if missing_parameters:
        raise ValueError(
            f"The following analysis parameters not found {', '.join(missing_parameters)} "
        )

    start_date = str(analysis_config["start_date"])
    end_date = str(analysis_config["end_date"])
    instruments_to_use = analysis_config["instruments_to_use"]
    WFTH = analysis_config["water_frequency_threshold_high"]
    WFTL = analysis_config["water_frequency_threshold_low"]
    PWT = analysis_config["permanent_water_threshold"]
    SC = analysis_config["sigma_coefficient"]

    gridspec = WaterbodiesGrid().gridspec
    # Parameters for dark pixel correction
    dp_adjust = {
        "msi_agm": {
            "ref_var": "msi12_agm",
            "var_list": [
                "msi04_agm",
                "msi03_agm",
                "msi02_agm",
                "msi05_agm",
                "msi06_agm",
                "msi07_agm",
            ],
        },
        "oli_agm": {
            "ref_var": "oli07_agm",
            "var_list": ["oli04_agm", "oli03_agm", "oli02_agm"],
        },
        "tm_agm": {
            "ref_var": "tm07_agm",
            "var_list": ["tm04_agm", "tm03_agm", "tm02_agm", "tm01_agm"],
        },
    }

    failed_tasks = []
    for idx, tile_idx in enumerate(tile_ids):
        log.info(f"Processing tile {tile_idx} {idx + 1} / {len(tile_ids)}")
        try:
            tile_geobox = gridspec.tile_geobox(
                tile_index=get_tile_index_int_tuple(tile_idx)
            )

            # don't try to use instruments for which there are no data
            instruments_to_use = check_instrument_dates(
                instruments_to_use, start_date, end_date
            )
            instruments_list = get_instruments_list(instruments_to_use)

            log.info("Building the multivariate/multi-sensor dataset")
            # build the multivariate/multi-sensor dataset.
            dc_queries = build_dc_queries(
                instruments_to_use, tile_geobox, start_date, end_date
            )
            ds = build_wq_dataset(dc_queries)

            log.info("Determining the pixels that are water")
            # Determine pixels that are water (sometimes, usually, permanent)
            ds = water_analysis(
                ds,
                water_frequency_threshold=WFTH,
                wofs_varname="wofs_ann_freq",
                permanent_water_threshold=PWT,
                sigma_coefficient=SC,
            )

            # Dark pixel correction
            ds = R_correction(ds, dp_adjust, instruments_to_use, WFTL)

            log.info("alculating the hue.")
            ds["hue"] = hue_calculation(ds, instrument="msi_agm")

            log.info("Determining the open water type for each pixel.")
            ds["owt_msi"] = OWT_pixel(ds, instrument="msi_agm")

            log.info("Applying the WQ algorithms to water areas.")
            # Apply the WQ algorithms to water areas, adding variables to the dataset and building
            # a list of water quality variable names
            # this can be run either keeping the wq variables as separate variables on the dataset,
            # or by moving them into new dimensions, 'tss' and 'chla'
            # If the arguments 'new_dimension_name' or 'new_varname' are None (or empty),
            # then the outputs will be retained as separate variables in a 3d dataset
            if (
                True
            ):  # put the data into a new dimension, call the variable 'tss' or 'chla'
                ds, tsm_vlist = WQ_vars(
                    ds.where(ds.wofs_ann_freq >= WFTL),
                    algorithms=ALGORITHMS_TSM,
                    instruments_list=instruments_list,
                    new_dimension_name="tss_measure",
                    new_varname="tss",
                )
                ds, chla_vlist = WQ_vars(
                    ds.where(ds.wofs_ann_freq >= WFTL),
                    algorithms=ALGORITHMS_CHLA,
                    instruments_list=instruments_list,
                    new_dimension_name="chla_measure",
                    new_varname="chla",
                )
            else:  # keep it simple, just add new data as new variables in a 3-D dataset
                ds, tsm_vlist = WQ_vars(
                    ds.where(ds.wofs_ann_freq >= WFTL),
                    algorithms=ALGORITHMS_TSM,
                    instruments_list=instruments_list,
                    new_dimension_name=None,
                    new_varname=None,
                )
                ds, chla_vlist = WQ_vars(
                    ds.where(ds.wofs_ann_freq >= WFTL),
                    algorithms=ALGORITHMS_CHLA,
                    instruments_list=instruments_list,
                    new_dimension_name=None,
                    new_varname=None,
                )
            wq_varlist = np.append(tsm_vlist, chla_vlist)

            keeplist = (
                "wofs_ann_clearcount",
                "wofs_ann_wetcount",
                "wofs_ann_freq",
                "wofs_ann_freq_sigma",
                "wofs_pw_threshold",
                "wofs_ann_pwater",
                "watermask",
                "owt_msi",
                "tss",
                "chla",
            )
            # the keeplist is not complete;
            # if the wq variables are retained as variables they will appear in a listing of data_vars.
            # therefore, revert to the instruments dictionary to list variables to drop
            droplist = []
            for instrument in list(instruments_list.keys()):
                for band in list(instruments_list[instrument].keys()):
                    variable = instruments_list[instrument][band]["varname"]
                    if variable not in keeplist:
                        droplist = np.append(droplist, variable)
                        droplist = np.append(droplist, variable + "r")
            for varname in droplist:
                if varname in ds.data_vars:
                    ds = ds.drop_vars(varname)

            ds["wofs_ann_confidence"] = (
                (1.0 - (ds.wofs_ann_freq_sigma / ds.wofs_ann_freq)) * 100
            ).astype("int16")

            parent_dir = join_url(output_directory, "WP1.4")
            output_file = join_url(
                parent_dir, f"wp12_ds_{tile_idx}_{start_date}_{end_date}.nc"
            )

            fs = get_filesystem(output_directory, anon=False)
            if not check_directory_exists(parent_dir):
                fs.makedirs(parent_dir, exist_ok=True)

            with fs.open(output_file, "wb") as f:
                ds.to_netcdf(f, engine="h5netcdf")

            log.info(f"Water Quality variables written to {output_file}")
        except Exception as error:
            log.exception(error)
            failed_tasks.append(tile_idx)

    if failed_tasks:
        failed_tasks_json_array = json.dumps(failed_tasks)

        tasks_directory = "/tmp/"
        failed_tasks_output_file = join_url(tasks_directory, "failed_tasks")

        fs = get_filesystem(path=tasks_directory, anon=False)
        if not check_directory_exists(path=tasks_directory):
            fs.mkdirs(path=tasks_directory, exist_ok=True)

        with fs.open(failed_tasks_output_file, "a") as file:
            file.write(failed_tasks_json_array + "\n")
        log.error(f"Failed tasks: {failed_tasks_json_array}")
        log.info(f"Failed tasks written to {failed_tasks_output_file}")
        sys.exit(1)
    else:
        sys.exit(0)


if __name__ == "__main__":
    cli()
